{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: Build your own NLP model\n",
    "For this challenge, you will need to choose a corpus of data from nltk or another source that includes categories you can predict and create an analysis pipeline that includes the following steps:\n",
    "\n",
    "    1.Data cleaning / processing / language parsing\n",
    "    \n",
    "    2.Create features using two different NLP methods: For example, BoW vs tf-idf.\n",
    "    \n",
    "    3.Use the features to fit supervised learning models for each feature set to predict the category outcomes.\n",
    "\n",
    "    4.Assess your models using cross-validation and determine whether one model performed better.\n",
    "\n",
    "    5.Pick one of the models and try to increase accuracy by at least 5 percentage points.\n",
    "\n",
    "Write up your report in a Jupyter notebook. Be sure to explicitly justify the choices you make throughout, and submit it below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves = gutenberg.raw('whitman-leaves.txt')\n",
    "stories = gutenberg.raw('bryant-stories.txt')\n",
    "busterbrown = gutenberg.raw('burgess-busterbrown.txt')\n",
    "mobydick = gutenberg.raw('melville-moby_dick.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "leaves = re.sub(r'Chapter \\d+', '', leaves)\n",
    "leaves = re.sub(r'[A-Z]{2,100}', '', leaves)\n",
    "\n",
    "stories = re.sub(r'CHAPTER .*', '', stories)\n",
    "stories = re.sub(r'[A-Z]{2,100}', '', stories)\n",
    "\n",
    "busterbrown = re.sub(r'Chapter \\d+', '', busterbrown)\n",
    "busterbrown = re.sub(r'\\w+\\t', '', busterbrown)\n",
    "\n",
    "mobydick = re.sub(r'CHAPTER .*', '', mobydick)\n",
    "mobydick = re.sub(r'CHAPTER \\w+', '', mobydick)\n",
    "\n",
    "\n",
    "leaves = text_cleaner(leaves[:int(len(leaves)/10)])\n",
    "stories = text_cleaner(stories[:int(len(stories)/10)])\n",
    "busterbrown = text_cleaner(busterbrown[:int(len(busterbrown)/10)])\n",
    "mobydick = text_cleaner(mobydick[:int(len(mobydick)/10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves = text_cleaner(leaves)\n",
    "stories = text_cleaner(stories)\n",
    "busterbrown = text_cleaner(busterbrown)\n",
    "mobydick = text_cleaner(mobydick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "leaves_doc = nlp(leaves)\n",
    "stories_doc = nlp(stories)\n",
    "busterbrown_doc = nlp(busterbrown)\n",
    "mobydick_doc = nlp(mobydick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group into sentences.\n",
    "leaves_sents = [[sent, \"Blake\"] for sent in leaves_doc.sents]\n",
    "stories_sents = [[sent,  \"Bryant\"] for sent in stories_doc.sents]\n",
    "busterbrown_sents = [[sent, \"Burgess\"] for sent in busterbrown_doc.sents]\n",
    "mobydick_sents = [[sent, \"Melville\"] for sent in mobydick_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(leaves_sents + stories_sents + busterbrown_sents + mobydick_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Come, ,, said, my, soul, ,, Such, verses, for...</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(That, should, I, after, return, ,, Or, ,, lon...</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(I, Sing, One's, -, self)</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(I, sing, ,, a, simple, separate, person, ,, Y...</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(En, -, Masse, .)</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0      1\n",
       "0  (Come, ,, said, my, soul, ,, Such, verses, for...  Blake\n",
       "1  (That, should, I, after, return, ,, Or, ,, lon...  Blake\n",
       "2                          (I, Sing, One's, -, self)  Blake\n",
       "3  (I, sing, ,, a, simple, separate, person, ,, Y...  Blake\n",
       "4                                  (En, -, Masse, .)  Blake"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the bags.\n",
    "leavesWords = bag_of_words(leaves_doc)\n",
    "storiesWords = bag_of_words(stories_doc)\n",
    "busterbrownWords = bag_of_words(busterbrown_doc)\n",
    "mobydickWords = bag_of_words(mobydick_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(leavesWords + storiesWords + busterbrownWords + mobydickWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "Processing row 150\n",
      "Processing row 200\n",
      "Processing row 250\n",
      "Processing row 300\n",
      "Processing row 350\n",
      "Processing row 400\n",
      "Processing row 450\n",
      "Processing row 500\n",
      "Processing row 550\n",
      "Processing row 600\n",
      "Processing row 650\n",
      "Processing row 700\n",
      "Processing row 750\n",
      "Processing row 800\n",
      "Processing row 850\n",
      "Processing row 900\n",
      "Processing row 950\n",
      "Processing row 1000\n",
      "Processing row 1050\n",
      "Processing row 1100\n",
      "Processing row 1150\n",
      "Processing row 1200\n",
      "Processing row 1250\n",
      "Processing row 1300\n",
      "Processing row 1350\n",
      "Processing row 1400\n",
      "Processing row 1450\n",
      "Processing row 1500\n",
      "Processing row 1550\n",
      "Processing row 1600\n",
      "Processing row 1650\n",
      "Processing row 1700\n",
      "Processing row 1750\n",
      "Processing row 1800\n",
      "Processing row 1850\n",
      "Processing row 1900\n",
      "Processing row 1950\n",
      "Processing row 2000\n",
      "Processing row 2050\n",
      "Processing row 2100\n",
      "Processing row 2150\n",
      "Processing row 2200\n",
      "Processing row 2250\n",
      "Processing row 2300\n",
      "Processing row 2350\n",
      "Processing row 2400\n",
      "Processing row 2450\n",
      "Processing row 2500\n",
      "Processing row 2550\n"
     ]
    }
   ],
   "source": [
    "word_counts = bow_features(sentences, common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>suicide</th>\n",
       "      <th>10,440</th>\n",
       "      <th>musical</th>\n",
       "      <th>inventor</th>\n",
       "      <th>os</th>\n",
       "      <th>orbic</th>\n",
       "      <th>convex</th>\n",
       "      <th>lovely</th>\n",
       "      <th>Till</th>\n",
       "      <th>Coffin</th>\n",
       "      <th>...</th>\n",
       "      <th>industry</th>\n",
       "      <th>fate</th>\n",
       "      <th>chair</th>\n",
       "      <th>desire</th>\n",
       "      <th>Librarian</th>\n",
       "      <th>physiognomy</th>\n",
       "      <th>look'd</th>\n",
       "      <th>til</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Come, ,, said, my, soul, ,, Such, verses, for...</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(That, should, I, after, return, ,, Or, ,, lon...</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(I, Sing, One's, -, self)</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(I, sing, ,, a, simple, separate, person, ,, Y...</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(En, -, Masse, .)</td>\n",
       "      <td>Blake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3660 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  suicide 10,440 musical inventor os orbic convex lovely Till Coffin  \\\n",
       "0       0      0       0        0  0     0      0      0    0      0   \n",
       "1       0      0       0        0  0     0      0      0    0      0   \n",
       "2       0      0       0        0  0     0      0      0    0      0   \n",
       "3       0      0       0        0  0     0      0      0    0      0   \n",
       "4       0      0       0        0  0     0      0      0    0      0   \n",
       "\n",
       "      ...     industry fate chair desire Librarian physiognomy look'd til  \\\n",
       "0     ...            0    0     0      0         0           0      0   0   \n",
       "1     ...            0    0     0      0         0           0      0   0   \n",
       "2     ...            0    0     0      0         0           0      0   0   \n",
       "3     ...            0    0     0      0         0           0      0   0   \n",
       "4     ...            0    0     0      0         0           0      0   0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (Come, ,, said, my, soul, ,, Such, verses, for...       Blake  \n",
       "1  (That, should, I, after, return, ,, Or, ,, lon...       Blake  \n",
       "2                          (I, Sing, One's, -, self)       Blake  \n",
       "3  (I, sing, ,, a, simple, separate, person, ,, Y...       Blake  \n",
       "4                                  (En, -, Masse, .)       Blake  \n",
       "\n",
       "[5 rows x 3660 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning on BOWs features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Y_bow = word_counts['text_source']\n",
    "X_bow = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(X_bow, \n",
    "                                                    Y_bow,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t BOWs Accuracy\n",
      "\t score\t  std\n",
      "NB:\t0.5865 (0.0352)\n",
      "KNN:\t0.5383 (0.0561)\n",
      "LR:\t0.7356 (0.0130)\n",
      "SVM:\t0.7123 (0.0150)\n",
      "RF:\t0.6957 (0.0165)\n",
      "DT:\t0.6585 (0.0213)\n",
      "GB:\t0.6973 (0.0152)\n",
      "LRTuned:\t0.7467 (0.0076)\n"
     ]
    }
   ],
   "source": [
    "#Let's see the accuracy scores for all of our models. \n",
    "models = []\n",
    "results = []\n",
    "names = []\n",
    "print('\\t BOWs Accuracy\\n\\t score\\t  std')\n",
    "\n",
    "models.append(('NB', GaussianNB()))\n",
    "\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "\n",
    "models.append(('LR', LogisticRegression()))\n",
    "\n",
    "models.append(('SVM', SVC(kernel='linear')))\n",
    "\n",
    "models.append(('RF', RandomForestClassifier()))\n",
    "\n",
    "models.append(('DT', DecisionTreeClassifier()))\n",
    "\n",
    "models.append(('GB', GradientBoostingClassifier()))\n",
    "models.append(('LRTuned', LogisticRegression(C=10,penalty= 'l2')))\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits = 5, random_state = 99)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train_bow, y_train_bow,\n",
    "                                                cv = 5, \n",
    "                                                 scoring = 'accuracy')\n",
    "   \n",
    "\n",
    "    msg = \"%s:\\t%.4f (%.4f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when using BOWs Logistic Regression explains the variance at 73.50%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF on sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer1=TfidfVectorizer(max_df=0.5, \n",
    "                            min_df=2, \n",
    "                            stop_words='english',\n",
    "                            lowercase=False,\n",
    "                            use_idf=True,\n",
    "                            #norm=u'13',\n",
    "                            smooth_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "#Take pandas series (text_sentence), convert from spacy object to string\n",
    "sentence1 = word_counts['text_sentence'].astype(str)\n",
    "print(type(sentence1))\n",
    "\n",
    "#Pass pandas series to our vectorizer model\n",
    "text_tfidf = vectorizer1.fit_transform(sentence1)\n",
    "print(type(text_tfidf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sing': 0.2776142230122457,\n",
       " 'simple': 0.3507970589256886,\n",
       " 'separate': 0.34031646700127266,\n",
       " 'person': 0.31824136686009585,\n",
       " 'Yet': 0.30328425255854324,\n",
       " 'utter': 0.36430889194788424,\n",
       " 'word': 0.5983950156333977}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#List of features\n",
    "terms = vectorizer1.get_feature_names()\n",
    "\n",
    "#Shape\n",
    "n = text_tfidf.shape[0]\n",
    "\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bysent = [{} for _ in range(0,n)]\n",
    "\n",
    "#for each sentence, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*text_tfidf.nonzero()):\n",
    "    tfidf_bysent[i][terms[j]] = text_tfidf[i, j]\n",
    "\n",
    "#Show first dictionary\n",
    "display(tfidf_bysent[3])\n",
    "print(type(tfidf_bysent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe for this feature set\n",
    "tfidf_df = pd.DataFrame(columns=terms)\n",
    "tfidf_df['text_sentence'] = word_counts['text_sentence']\n",
    "tfidf_df['text_source'] = word_counts['text_source']\n",
    "tfidf_df.loc[:, terms] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_tfidf = tfidf_df['text_source']\n",
    "X_tfidf = tfidf_df.drop(['text_sentence','text_source'], 1)\n",
    "\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf, \n",
    "                                                    Y_tfidf,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning on TDFIDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t TFIDF Accuracy\n",
      "\t score\t  std\n",
      "NB:\t0.2533 (0.0008)\n",
      "KNN:\t0.5405 (0.0017)\n",
      "LR:\t0.5405 (0.0017)\n",
      "SVM:\t0.5405 (0.0017)\n",
      "RF:\t0.5405 (0.0017)\n",
      "DT:\t0.5405 (0.0017)\n",
      "GB:\t0.5405 (0.0017)\n",
      "LRTuned:\t0.5405 (0.0017)\n"
     ]
    }
   ],
   "source": [
    "#Let's see the accuracy scores for all of our models. \n",
    "models1 = []\n",
    "results1 = []\n",
    "names1 = []\n",
    "print('\\t TFIDF Accuracy\\n\\t score\\t  std')\n",
    "\n",
    "models1.append(('NB', GaussianNB()))\n",
    "\n",
    "models1.append(('KNN', KNeighborsClassifier()))\n",
    "\n",
    "models1.append(('LR', LogisticRegression(C=1e9)))\n",
    "\n",
    "models1.append(('SVM', SVC(kernel='linear')))\n",
    "\n",
    "models1.append(('RF', RandomForestClassifier()))\n",
    "\n",
    "models1.append(('DT', DecisionTreeClassifier()))\n",
    "\n",
    "models1.append(('GB', GradientBoostingClassifier()))\n",
    "models1.append(('LRTuned', LogisticRegression(C=10,penalty= 'l2')))\n",
    "\n",
    "for name1, model1 in models1:\n",
    "    kfold1 = model_selection.KFold(n_splits = 5, random_state = 99)\n",
    "    cv_results1 = model_selection.cross_val_score(model1, X_train_tfidf, y_train_tfidf,\n",
    "                                                cv = 5, \n",
    "                                                 scoring = 'accuracy')\n",
    "   \n",
    "\n",
    "    msg1 = \"%s:\\t%.4f (%.4f)\" % (name1, cv_results1.mean(), cv_results1.std())\n",
    "    print(msg1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Logistic Regression will be the model to try and increase the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import GridseachCV to find the best parameters for Logistic Regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Instantiate\n",
    "clf = LogisticRegression()\n",
    "\n",
    "#Grid\n",
    "parameter_grid = {'C': [0.01, 0.1, 1, 2, 10, 100], 'penalty': ['l1', 'l2']}\n",
    "\n",
    "#Gridsearch\n",
    "gridsearch = GridSearchCV(clf, parameter_grid)\n",
    "gridsearch.fit(X_train_bow, y_train_bow);\n",
    "\n",
    "#Get best hyperparameters\n",
    "gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "BOWs was better at predicting than TFIDF.  Logistic Regression was the best algorithm.  I found the best parameters using gridsearch but I was only able to increase the prediction by 1.11%. \n",
    "\n",
    "I will come back to this and use gridsearch for the other algorithms to see if I can increase the accuracy by more than 5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
